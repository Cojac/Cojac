@misc{COJAC,
    title = {COJAC - Boosting arithmetic capabilities of Java numbers},
    year = {2019},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/Cojac/Cojac}},
    commit = {853e536204aa71f73b5f5844b60827e2c9cebbe4}
}
@article{posit,
    author = {Gustafson and Yonemoto},
    title = {Beating Floating Point at Its Own Game: Posit Arithmetic},
    year = {2017},
    issue_date = {June 2017},
    publisher = {South Ural State University},
    address = {Chelyabinsk, RUS},
    volume = {4},
    number = {2},
    issn = {2409-6008},
    url = {https://doi.org/10.14529/jsfi170206},
    doi = {10.14529/jsfi170206},
    abstract = {A new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers floats. Unlike earlier forms of universal number unum arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and "Nota-Number" NaN indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second POPS supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality. A comprehensive series of benchmarks compares floats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than "approximate computing" methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than floats of the same size; in some cases, a 32-bit posit may safely replace a 64-bit float. In other words, posits beat floats at their own game.},
    journal = {Supercomput. Front. Innov.: Int. J.},
    month = jun,
    pages = {71â€“86},
    numpages = {16},
    keywords = {floating point, neural networks, posits, valid arithmetic, unum computing, linear algebra, LINPACK, computer arithmetic, energy-efficient computing}
}